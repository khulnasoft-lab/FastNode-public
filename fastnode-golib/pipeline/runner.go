package pipeline

import (
	"fmt"
	"log"
	"os"
	"sync"
	"time"

	"github.com/khulnasoft-lab/fastnode/fastnode-golib/workerpool"
)

// RunState describes the current state of the runner
type RunState string

const (
	// StateWaiting is the default state; the runner is waiting to start
	StateWaiting RunState = "waiting"
	// StateRunning is set when the runner is currently running the pipeline
	StateRunning RunState = "running"
	// StateFinished is reached once the runner finishes, either successfully or with an error. GetResults
	// can be used to get the results.
	StateFinished RunState = "finished"
)

// runner is responsible for executing the pipeline and getting the results
type runner struct {
	clone       PipeClone
	opts        EngineOptions
	shard       int
	totalShards int

	workers []worker

	// done is closed when the runner finishes executing
	done chan struct{}

	state     RunState
	startTime time.Time
	results   map[Aggregator]Sample
	err       error
	m         sync.Mutex

	startedAt  time.Time
	finishedAt time.Time
	stats      runStats
}

func newRunner(pipe Pipeline, shard int, totalShards int, opts EngineOptions) (*runner, error) {
	log.Printf("starting new runner, shard = %d/%d, numWorkers = %d", shard, totalShards, opts.NumWorkers)

	// we clone the sources of the pipeline since now we know which shard of the data to generate
	clone, err := pipe.CloneForShard(shard, totalShards)
	if err != nil {
		return nil, err
	}

	r := &runner{
		clone:       clone,
		opts:        opts,
		shard:       shard,
		totalShards: totalShards,
		done:        make(chan struct{}),

		state: StateWaiting,
		stats: newRunStats(),
	}

	r.printDependents()

	workers := make([]worker, 0, opts.NumWorkers)
	for t := 0; t < opts.NumWorkers; t++ {
		w, err := newWorker(r)
		if err != nil {
			return nil, err
		}
		workers = append(workers, w)
	}
	r.workers = workers

	return r, nil
}

// Start running the pipeline in the background
func (r *runner) Start() error {
	type task struct {
		Record Record
		Source Source
	}

	// tasks generated by the sources
	tasks := make(chan task)

	// For each source, maintain a goroutine that pulls records from the source until it's done
	recordGen := workerpool.New(len(r.clone.Sources))

	r.setStarted()

	for _, s := range r.clone.Sources {
		source := s

		okList := r.opts.OnlyKeys[s.Name()]
		onlyKeys := make(map[string]struct{}, len(okList))
		for _, k := range okList {
			onlyKeys[k] = struct{}{}
		}

		recordGen.Add([]workerpool.Job{
			func() error {
				for {
					rec := source.SourceOut()
					if rec == (Record{}) {
						return nil
					} else if ns, ok := rec.Value.(sampleError); ok {
						r.stats.AddFeedError(source, source.Name(), rec.Key, ns)
					} else {
						if len(onlyKeys) > 0 {
							if _, found := onlyKeys[rec.Key]; !found {
								r.stats.AddFeedError(source, source.Name(), rec.Key,
									NewError("filtered out").(sampleError))
								continue
							}
						}
						tasks <- task{Record: rec, Source: source}
						r.stats.IncrFeedOut(source)
					}
				}
			},
		})
	}

	// Maintain a pool of workers that analyzes the records generated by the sources
	analysis := workerpool.New(len(r.workers))

	for _, w := range r.workers {
		worker := w
		analysis.Add([]workerpool.Job{
			func() error {
				for {
					select {
					case t, ok := <-tasks:
						if !ok {
							return nil
						}
						worker.Run(t.Source, t.Record)
					case <-r.done:
						return nil
					}
				}
			},
		})
	}

	// periodically print out the stats
	go func() {
		for {
			status := r.Status()
			if status.State == StateFinished {
				return
			}

			log.Printf("time since start: %v, runner stats:", time.Since(r.startTime))
			printStats(os.Stderr, r.stats.Stats())
			time.Sleep(30 * time.Second)
		}
	}()

	// Wait for the execution to finish in the background
	go func() {
		// First, wait until all sources are done generating records
		err := recordGen.Wait()
		close(tasks)
		if err != nil {
			r.setError(fmt.Errorf("error waiting for recordGen pool: %v", err))
			return
		}

		// Then, wait for all the workers to finish analyzing the records
		if err := analysis.Wait(); err != nil {
			r.setError(fmt.Errorf("error waiting for analysis pool: %v", err))
			return
		}

		log.Printf("final runner stats:")
		printStats(os.Stderr, r.stats.Stats())

		// aggregate the results of the aggregators
		aggs := r.clone.Aggregators()
		results := make(map[Aggregator]Sample, len(aggs))

		for _, agg := range aggs {
			log.Printf("aggregating results of %s", agg.Name())

			clones := make([]Aggregator, 0, len(r.workers))
			for _, w := range r.workers {
				clones = append(clones, w.ClonedAggregator(agg))
			}

			out, err := agg.AggregateLocal(clones)
			if err != nil {
				r.setError(fmt.Errorf("error aggregating results of %s: %v", agg.Name(), err))
				return
			}
			// we set the result fore the original aggregator because that is the one in the original pipeline
			// definition
			originalAgg := r.clone.CloneToOrig[agg].(Aggregator)
			results[originalAgg] = out
		}

		r.setResults(results)
	}()

	return nil
}

// Wait until the runner has finished
func (r *runner) Wait() {
	<-r.done
}

type runStatus struct {
	State      RunState
	Err        error
	StartedAt  time.Time
	FinishedAt time.Time
}

// Status returns the current state of the runner and whether an error has been encountered so far
func (r *runner) Status() runStatus {
	r.m.Lock()
	defer r.m.Unlock()

	return runStatus{
		State:      r.state,
		StartedAt:  r.startedAt,
		FinishedAt: r.finishedAt,
		Err:        r.err,
	}
}

// GetResults after the pipeline has finished executing
func (r *runner) GetResults() (map[Aggregator]Sample, error) {
	r.m.Lock()
	defer r.m.Unlock()

	if r.results == nil {
		return nil, fmt.Errorf("no results present")
	}
	return r.results, nil
}

func (r *runner) setStarted() {
	r.m.Lock()
	defer r.m.Unlock()

	r.startTime = time.Now()
	r.state = StateRunning
	r.startedAt = time.Now().UTC()
}

// setResults sets the final results after the pipeline has successfully executed.
// If the state was already marked as finished, ignore.
func (r *runner) setResults(results map[Aggregator]Sample) {
	r.m.Lock()
	defer r.m.Unlock()

	if r.state == StateFinished {
		return
	}

	log.Printf("runner setting results, len(results) = %d", len(results))

	r.state = StateFinished
	r.finishedAt = time.Now().UTC()
	r.results = results
	close(r.done)
}

// setError sets an error encountered during the execution of the runner's pipeline.
// If the state was already marked as finished, ignore.
func (r *runner) setError(err error) {
	r.m.Lock()
	defer r.m.Unlock()

	if r.state == StateFinished {
		log.Printf("runner encountered subsequent error: %v", err)
		return
	}

	log.Printf("runner encountered first error: %v", err)
	r.state = StateFinished
	r.finishedAt = time.Now().UTC()
	r.err = err
	close(r.done)
}

func (r *runner) printDependents() {
	log.Printf("Dependents:")
	for k, v := range r.clone.Dependents {
		log.Printf("%s:", k.Name())
		for _, d := range v {
			log.Printf("  => %s", d.Name())
		}
	}
}
