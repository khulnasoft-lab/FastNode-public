{
    "Buffer": "state_capitals = {\n    'Maryland': 'Annapolis',\n    'Massachusetts': 'Boston',\n    'Montana': 'Who Cares?',\n    'New Hampshire': 'Concord'\n}\n\n\ndef dict_flip(states):\n    rev_states = {v: k for k, v in states.items()}\n    print('Capital: {} State: {}'.format(items(rev))\n\n\ndef dict_lookup(states):\n    for k, v in states.items():\n        print('State: {}\\nCapital: {}\\n\\n'.format(k, v))\n\n\ndict_lookup(state_capitals)\ndict_flip(state_capitals)\n",
    "Cursor": 208,
    "Symbol": "__builtin__.dict",
    "Expected": "items",
    "Provided": null
}
{
    "Buffer": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nfrom syslog import syslog\nimport django\n\n\ndef main():\n\n    from vector.zoll.views import import_abbyy_auftrag\n\n    syslog('Import ZOM gestartet')\n    return import_abbyy_auftrag(doc_type='t2'), \\\n           import_abbyy_auftrag(doc_type='abd'), \\\n           import_abbyy_auftrag(doc_type='sivo')\n\n\nif __name__ == \"__main__\":\n    django.setup()\n    from django.conf import settings\n    DEBUGMODE = settings.DEBUG\n    # return_value = 0\n    try:\n        ret1, ret2, ret3 = main()\n    except Exception as exp:\n        if DEBUGMODE:\n            raise\n        else:\n            syslog('Fehler beim ZOM Import: {}'.format(exp))\n            sys.exit()\n    syslog('ZOM Auftrag erfolgreich angelegt')\n    sys.exit(0)\n",
    "Cursor": 394,
    "Symbol": "django",
    "Expected": "setup",
    "Provided": null
}
{
    "Buffer": "import os\nimport json\nfrom typing import Dict\nfrom pathlib import Path\n\nimport logbook\n\nfrom kryptos.settings import PROJECT_ID\n\n\nfrom google.cloud import storage, kms_v1\n\nkey_client = kms_v1.KeyManagementServiceClient()\nstorage_client = storage.Client()\n\nlog = logbook.Logger(\"ExchangeAuth\")\n\ndef get_auth_alias_path(user_id: str, exchange_name: str) -\u003e str:\n    home_dir = str(Path.home())\n    exchange_dir = os.path.join(home_dir, \".catalyst/data/exchanges/\", exchange_name.lower())\n    user_file = f\"auth{user_id}.json\"\n    file_name = os.path.join(exchange_dir, user_file)\n    return file_name\n\n\ndef decrypt_auth_key(user_id: int, exchange_name: str, ciphertext: bytes) -\u003e Dict[str, str]:\n    \"\"\"Decrypts auth data using google cloud KMS\n\n    Args:\n        user_id (int)\n        exchange_name (str)\n        ciphertext (bytes): encrypted data\n\n    Returns:\n        Dict[str, str]: Description\n    \"\"\"\n    log.debug('decrypting exchange auth')\n    key_path = key_client.crypto_key_path_path(\n        PROJECT_ID, \"global\", \"exchange_auth\", f\"{exchange_name}_{user_id}_key\"\n    )\n\n    response = key_client.decrypt(key_path, ciphertext)\n    log.info(f'successfully decrypted user {user_id} {exchange_name} auth')\n    return json.loads(response.plaintext)\n\n\ndef get_encrypted_auth(user_id: int, exchange_name: str) -\u003e bytes:\n    \"\"\"Fetches encrypted auth data as blob from storage bucket\n\n    Args:\n        user_id (int): Description\n        exchange_name (str): Description\n\n    Returns:\n        bytes: ciphertext - encrypted auth json\n    \"\"\"\n    log.debug(\"Fetching encrypted user exchange auth from storage\")\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(\"catalyst_auth\")\n    blob = bucket.blob(f\"auth_{exchange_name}_{user_id}_json\")\n    encrypted_text = blob.download_as_string()\n    log.info(\"obtained encrypted auth\")\n    return encrypted_text\n\n\ndef save_to_catalyst(user_id: int, exchange_name: str, auth_dict: Dict[str, str]) -\u003e None:\n    \"\"\"Saves decrypted auth data to catalyst dir\"\"\"\n    file_name = get_auth_alias_path()\n    os.makedirs(exchange_dir, exist_ok=True)\n\n    with open(file_name, \"w\") as f:\n        log.warn(f\"Writing auth_json_str to {file_name}\")\n        json.dump(auth_dict, f)\n\n\ndef get_user_auth_alias(user_id: int, exchange_name: str) -\u003e Dict[str, str]:\n    \"\"\"Fetches user exchange auth data and returns the catalyst auth alias\n\n    Args:\n        user_id (int): strategy's user ID\n        exchange_name (str): name of exchange to to authenticate\n\n    Returns:\n        str: auth alias specifying json file for catalyst to use\n\n    Returns:\n        Dict[str, str]: auth alias specifying file for catalyst to use\n    \"\"\"\n    encrypted = get_encrypted_auth(user_id, exchange_name)\n    auth_dict = decrypt_auth_key(user_id, exchange_name, encrypted)\n    save_to_catalyst(user_id, exchange_name, auth_dict)\n    auth_alias = {exchange_name: f\"auth{user_id}\"}\n\n    return auth_alias\n\ndef delete_alias_file(user_id: int, exchange_name: str) -\u003e None:\n    log.debug(f\"Deleting user {user_id}'s {exchange_name} auth alias file\")\n    home_dir = str(Path.home())\n    exchange_dir = os.path.join(home_dir, \".catalyst/data/exchanges/\", exchange_name.lower())\n    user_file = f\"auth{user_id}.json\"\n    file_name = os.path.join(exchange_dir, user_file)\n    os.remove(file_name)\n    assert not os.path.exists(file_name)\n",
    "Cursor": 548,
    "Symbol": "os.path",
    "Expected": "join",
    "Provided": null
}
{
    "Buffer": "guests_list = ['Kennady', 'Eminem', 'Jobs']\r\nfor guest in guests_list:\r\n  message = \"Hello, \" + guest + \". I'd like to dinner with you.\"\r\n  print(message)\r\nprint(\"\\nThe list of guest become bigger now.\\n\")\r\nguests_list.insert(0, 'L-Jay')\r\nguests_list.insert(2, 'Urgant')\r\nguests_list.append('August Ames')\r\nfor guest in guests_list:\r\n  message = \"Hello, \" + guest + \". I'd like to dinner with you.\"\r\n  print(message)\r\nprint(\"It will be dinner for only two persones.\")\r\nprint(guests_list)\r\nfor guest in reversed(guests_list):\r\n\tif len:\r\n\t\tpass\r\n  popped_guest = guests_list.pop()\r\n  print(\"I'm sorry, but \" + popped_guest + \" couldn't come to the dinner\")\r\nprint(guests_list)\r\n",
    "Cursor": 573,
    "Symbol": "__builtin__.list",
    "Expected": "pop",
    "Provided": null
}
{
    "Buffer": "from mtcnn.mtcnn import MTCNN\n\nimport numpy as np\nimport cv2\nfrom IPython import embed\n# face detector\ndetector = MTCNN()\n\n# open video stream\ncap = cv2.VideoCapture(0)\n\n\n# start the capture...\nwhile(True):\n    # Capture frame-by-frame\n    ret, frame = cap.read()\n\n    detections = detector.detect_faces(frame)\n\n    if len(detections)\n    print(detections)\n    embed()\n    bbx = detections['box'] # [381, 155, 675, 872]\n    print(bbx)\n    print(frame.shape)\n    x, y, x1, y2 = bbx\n    frame_visualize = cv2.rectangle(frame,  (x, y), (x+w, y+h))\n    # Our operations on the frame come here\n    # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Display the resulting frame\n    cv2.imshow('frame',frame_visualize)\n    if cv2.waitKey(1) \u0026 0xFF == ord('q'):\n        break\n\n# When everything done, release the capture\ncap.release()\ncv2.destroyAllWindows()\n",
    "Cursor": 685,
    "Symbol": "cv2",
    "Expected": "imshow",
    "Provided": null
}
{
    "Buffer": "import cv2\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\nimport os\r\n\r\n\r\nimg = cv2.imread('LNG_P\u0026ID2', 0)\r\nimg2 = img.copy()",
    "Cursor": 95,
    "Symbol": "cv2",
    "Expected": "imread",
    "Provided": null
}
{
    "Buffer": "import pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib, os\r\n\r\ndata = pd.read_csv(\"../data.csv\", index_col=\"Name\")\r\ninfo = pd.read_csv(\"../info.csv\", index_col=\"Name\")\r\nMAX = int(np.ceil(data[\"Online\"].max() / 100.0)) * 100\r\n\r\ndata[\"Hour\"] = data[\"Date\"].apply(lambda x: x.split(\" \")[1])\r\ndata[\"Date\"] = data[\"Date\"].apply(lambda x: x.split(\" \")[0])\r\n\r\ndata[\"Hour\"] = data[\"Hour\"].apply(lambda x: x.split(\":\")[0]).apply(np.int64)\r\ndata[\"Day\"] = data[\"Date\"].apply(lambda x: x.split(\"/\")[0]).apply(np.int64)\r\ndata[\"Month\"] = data[\"Date\"].apply(lambda x: x.split(\"/\")[1]).apply(np.int64)\r\ndata[\"Year\"] = data[\"Date\"].apply(lambda x: x.split(\"/\")[2]).apply(np.int64)\r\n\r\ndata = data.drop(\"Date\", axis=1)\r\n\r\nworlds = []\r\nfor world, inf in info.iterrows():\r\n    df = data.loc[world]\r\n    groupby = df.groupby([df.index, \"Hour\"])\r\n    df = groupby[\"Online\"].agg([\"mean\", \"std\"]).fillna(0)\r\n    worlds.append(df.reset_index(\"Hour\"))\r\n\r\nfor file in os.listdir(\"graphs\"):\r\n    os.remove(\"graphs/{}\".format(file))\r\n\r\ncolors_in = {\"Open PvP\": '#b9b313', \"Retro Open PvP\": '#b9b313',\r\n             \"Optional PvP\": \"#66b3a9\", \"Hardcore PvP\": \"#ec7161\",\r\n             \"Retro Hardcore PvP\": \"#ec7161\"}\r\n\r\nplt.style.use('seaborn-whitegrid')\r\nfig = plt.figure()\r\nax = plt.axes()\r\n\r\nstack = True\r\n\r\nfor world in worlds:\r\n    fig.set_size_inches(25, 10, forward=True)\r\n    world_name = world.index[0]\r\n    inf = info.loc[world_name]\r\n\r\n    if inf[\"Location\"] != \"Europe\" and (inf[\"PVP Type\"] != \"Optional PvP\" or \"blocked\" in inf[\"Notes\"](:\r\n        continue\r\n\r\n    title = \"{} - {}\".format(world_name, inf[\"Location\"])\r\n    if inf[\"Notes\"] != \" \":\r\n        title += \" ({})\".format(inf[\"Notes\"])\r\n\r\n    plt.plot(np.arange(24), np.zeros(24)+500, lw=2, c='black', alpha=0.3)\r\n    if not stack:\r\n        plt.plot(world[\"Hour\"], world[\"mean\"], c=colors_in[inf[\"PVP Type\"]], marker=\".\", ms=10, lw=2, label=world_name)\r\n    else:\r\n        plt.plot(world[\"Hour\"], world[\"mean\"], marker=\".\", ms=10, lw=2, label=world_name)\r\n\r\n    if not stack:\r\n        plt.title(title)\r\n        plt.axvspan(0, 2, color='green', alpha=0.1)\r\n        plt.axvspan(8, 12, color='yellow', alpha=0.2)\r\n        plt.axvspan(16, 19, color='yellow', alpha=0.2)\r\n        plt.axvspan(21, 23, color='green', alpha=0.1)\r\n\r\n        plt.xlabel(\"Hora\")\r\n        plt.ylabel(\"Promedio de players online\");\r\n\r\n        plt.ylim(0, MAX)\r\n        plt.xlim(0, 23)\r\n\r\n        plt.yticks(np.arange(0, MAX+100, 100))\r\n        plt.xticks(np.arange(24))\r\n\r\n        matplotlib.rc('font', family='Source Sans Pro Light', size=15)\r\n\r\n        plt.savefig(\"graphs/{}.png\".format(world_name.lower()), dpi=300)\r\n        plt.clf()\r\n\r\nif stack:\r\n    # filtro\r\n    filt = \"North America, Optional PVP\"\r\n\r\n    ax.legend(loc=\"best\")\r\n    plt.title(\"Comparisson - {}\".format(filt))\r\n    plt.axvspan(0, 2, color='green', alpha=0.1)\r\n    plt.axvspan(8, 12, color='yellow', alpha=0.2)\r\n    plt.axvspan(16, 19, color='yellow', alpha=0.2)\r\n    plt.axvspan(21, 23, color='green', alpha=0.1)\r\n\r\n    plt.xlabel(\"Hora\")\r\n    plt.ylabel(\"Promedio de players online\");\r\n\r\n    plt.ylim(0, MAX)\r\n    plt.xlim(0, 23)\r\n\r\n    plt.yticks(np.arange(0, MAX+100, 100))\r\n    plt.xticks(np.arange(24))\r\n\r\n    matplotlib.rc('font', family='normal', size=15)\r\n\r\n    plt.savefig(\"graphs/stack.png\".format(world_name.lower()), dpi=300)\r\n    plt.clf()\r\n",
    "Cursor": 1723,
    "Symbol": "matplotlib.pyplot",
    "Expected": "plot",
    "Provided": null
}
{
    "Buffer": "#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport sys\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport math\r\n\r\n# input\r\na = 5\r\nb = 0\r\nc = 0\r\n\r\nalpha = [x for x in range(-8, 9)]\r\nbeta = []\r\nfor x in alpha:\r\n    beta.append(a * x**2 + b * x + c)\r\n\r\nx1 = -b/a/2 + math.sqrt(())\r\n\r\nplt.plot(alpha, beta)\r\n\r\nplt.xlabel('x')\r\nplt.ylabel('y')\r\nplt.title('{}x**2 + {}x + {}'.format(a, b, c))\r\nplt.show()\r\n",
    "Cursor": 343,
    "Symbol": "matplotlib.pyplot",
    "Expected": "ylabel",
    "Provided": null
}
{
    "Buffer": "# encoding: UTF-8\n# Copyright 2016 Google.com\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tensorflow as tf\nimport tensorflowvisu\nimport math\nimport mnistdata\nprint(\"Tensorflow version \" + tf.__version__)\ntf.set_random_seed(0)\n\n# neural network with 5 layers\n#\n# · · · · · · · · · ·          (input data, flattened pixels)       X [batch, 784]   # 784 = 28*28\n# \\x/x\\x/x\\x/x\\x/x\\x/       -- fully connected layer (relu)         W1 [784, 200]      B1[200]\n#  · · · · · · · · ·                                                Y1 [batch, 200]\n#   \\x/x\\x/x\\x/x\\x/         -- fully connected layer (relu)         W2 [200, 100]      B2[100]\n#    · · · · · · ·                                                  Y2 [batch, 100]\n#     \\x/x\\x/x\\x/           -- fully connected layer (relu)         W3 [100, 60]       B3[60]\n#      · · · · ·                                                    Y3 [batch, 60]\n#       \\x/x\\x/             -- fully connected layer (relu)         W4 [60, 30]        B4[30]\n#        · · ·                                                      Y4 [batch, 30]\n#         \\x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]\n#          ·                                                        Y5 [batch, 10]\n\n# Download images and labels into mnist.test (10K images+labels) and mnist.train (60K images+labels)\nmnist = mnistdata.read_data_sets(\"data\", one_hot=True, reshape=False)\n\n# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\n# correct answers will go here\nY_ = tf.placeholder(tf.float32, [None, 10])\n# step for variable learning rate\nstep = tf.placeholder(tf.int32)\n\n# five layers and their number of neurons (tha last layer has 10 softmax neurons)\nL = 200\nM = 100\nN = 60\nO = 30\n# Weights initialised with small random values between -0.2 and +0.2\n# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\nW1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\nB1 = tf.Variable(tf.ones([L])/10)\nW2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\nB2 = tf.Variable(tf.ones([M])/10)\nW3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\nB3 = tf.Variable(tf.ones([N])/10)\nW4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\nB4 = tf.Variable(tf.ones([O])/10)\nW5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\nB5 = tf.Variable(tf.zeros([10]))\n\n# The model\nXX = tf.reshape(X, [-1, 784])\nY1 = tf.nn.relu(tf.matmul(XX, W1) + B1)\nY2 = tf.nn.relu(tf.matmul(Y1, W2) + B2)\nY3 = tf.nn.relu(tf.matmul(Y2, W3) + B3)\nY4 = tf.nn.relu(tf.matmul(Y3, W4) + B4)\nYlogits = tf.matmul(Y4, W5) + B5\nY = tf.nn.softmax(Ylogits)\n\n# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n# problems with log(0) which is NaN\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\ncross_entropy = tf.reduce_mean(cross_entropy)*100\n\n# accuracy of the trained model, between 0 (worst) and 1 (best)\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# matplotlib visualisation\nallweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\nallbiases  = tf.concat([tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])], 0)\nI = tensorflowvisu.tf_format_mnist_images(X, Y, Y_)\nIt = tensorflowvisu.tf_format_mnist_images(X, Y, Y_, 1000, lines=25)\ndatavis = tensorflowvisu.MnistDataVis()\n\n# training step\n# the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/2000)), i.e. exponential decay from 0.003-\u003e0.0001\nlr = 0.0001 +  tf.train.exponential_decay(0.003, step, 2000, 1/math.e)\ntrain_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n\n# init\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\n\n# You can call this function in a loop to train the model, 100 images at a time\ndef training_step(i, update_test_data, update_train_data):\n\n    # training on batches of 100 images with 100 labels\n    batch_X, batch_Y = mnist.train.next_batch(100)\n\n    # compute training values for visualisation\n    if update_train_data:\n        a, c, im, w, b, l = sess.run([accuracy, cross_entropy, I, allweights, allbiases, lr],\n                                     feed_dict={X: batch_X, Y_: batch_Y, step: i})\n        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(l) + \")\")\n        datavis.append_training_curves_data(i, a, c)\n        datavis.update_image1(im)\n        datavis.append_data_histograms(i, w, b)\n\n    # compute test values for visualisation\n    if update_test_data:\n        a, c, im = sess.run([accuracy, cross_entropy, It], feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n        datavis.append_test_curves_data(i, a, c)\n        datavis.update_image2(im)\n\n    # the backpropagation training step\n    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y, step: i})\n\ndatavis.animate(training_step, iterations=10000+1, train_data_update_freq=20, test_data_update_freq=100, more_tests_at_start=True)\n\n# to save the animation as a movie, add save_movie=True as an argument to datavis.animate\n# to disable the visualisation use the following line instead of the datavis.animate line\n# for i in range(10000+1): training_step(i, i % 100 == 0, i % 20 == 0)\n\nprint(\"max test accuracy: \" + str(datavis.get_max_test_accuracy()))\n\n# Some results to expect:\n# (In all runs, if sigmoids are used, all biases are initialised at 0, if RELUs are used,\n# all biases are initialised at 0.1 apart from the last one which is initialised at 0.)\n\n## learning rate = 0.003, 10K iterations\n# final test accuracy = 0.9788 (sigmoid - slow start, training cross-entropy not stabilised in the end)\n# final test accuracy = 0.9825 (relu - above 0.97 in the first 1500 iterations but noisy curves)\n\n## now with learning rate = 0.0001, 10K iterations\n# final test accuracy = 0.9722 (relu - slow but smooth curve, would have gone higher in 20K iterations)\n\n## decaying learning rate from 0.003 to 0.0001 decay_speed 2000, 10K iterations\n# final test accuracy = 0.9746 (sigmoid - training cross-entropy not stabilised)\n# final test accuracy = 0.9824 (relu - training set fully learned, test accuracy stable)\n\nimport numpy as np\ntargets = np.random.randint(3, size = 100)\ntargets\npreds = np.random.randint(10, size = 100)\npreds\nnp.concatenate((a, b), axis=2)\n\nresults = np.power(preds - targets, 2)\n\nreshaped_results = np.reshape(results, (20, 5))\nnp.mean(np.mean(reshaped_results, axis = 1))\n\nnp.sqrt(np.mean(np.power(preds - targets, 2))\n\n\n",
    "Cursor": 2822,
    "Symbol": "tensorflow",
    "Expected": "Variable",
    "Provided": null
}
{
    "Buffer": "import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\ndef d_pop(pop, r, K):\r\n    \"\"\"\r\n    Takes a population, a groth rate, and a carrying capacity.\r\n    Output the population increase.\r\n    \"\"\"\r\n    d_pop = (r*pop*(1-pop/K))\r\n    return (d_pop)\r\n\r\n# The total time in days it takes to make\r\ntimeperiod = 365*5\r\n\r\n\r\ng_cons = 4\r\nT_cons = 7\r\n\r\nfood = 25000\r\nrg = 1.0958904109589041\r\nrT = 1.006\r\ng = 100000\r\nT = 300\r\n\r\n########### Things for the iteration\r\nGoat = [] #List to store values of goats in\r\nTortoise = [] # List to store values of tortoises in\r\ntime = np.arange(timeperiod)\r\nt0 = 0.1\r\n\r\n\r\nfor t in time:\r\n    food_g = (g/(g+T))*food\r\n    Kg = g_cons*food_g\r\n\r\n    food_T = (T/(g+T))*food\r\n    KT = food_T*T_cons\r\n\r\n    # Kg = 100000\r\n    # KT = 300\r\n\r\n    T = T + d_pop(T, rT, KT)*(t-t0)\r\n\r\n    g = g + np.abs(d_pop(g, rg, Kg)*(t-t0) - 0.01*g**2\r\n\r\n    t0 = t\r\n\r\n    Goat.append(g)\r\n    Tortoise.append(T)\r\n\r\n\r\n\r\n\r\n\r\n####################\r\n# Matplotlib stuff behind here\r\n####################\r\ngoatplt = plt.plot(time, Goat, \"b\")\r\ntortplt = plt.plot(time, Tortoise, \"r\")\r\nplt.show()\r\n",
    "Cursor": 1018,
    "Symbol": "matplotlib.pyplot",
    "Expected": "plot",
    "Provided": null
}
{
    "Buffer": "import requests\n\ndef get_joke(topic):\n\turl = f\"https://icanhazdadjoke.com/search?limit=1\u0026term={topic}\"\n\tresponse = requests.get(url, headers={\"Accept\": \"application/json\"})\n\tres_format = response.json()\n\treturn res_format[\"results\"].pop(\n\n\ndef main():\n\n\tprint(\"Welcome. Get ready for a joke!\")\n\tprint(get_joke(\"dad\"))\n\tanswer = \"\"\n\twhile answer != \"N\":\n\t\tanswer = input(\"Want another joke? Y/N \").upper()\n\t\tif answer == \"Y\":\n\t\t\tprint(get_joke(\"dad\"))\n\t\telif answer != \"N\":\n\t\t\tprint(\"Please answer with Y or N only'\")\n\tprint(\"Thx, bye!\")\n\n\nprint(get_joke(\"dad\"))",
    "Cursor": 124,
    "Symbol": "requests",
    "Expected": "get",
    "Provided": null
}
{
    "Buffer": "# CS231A Homework 0, Problem 4\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import misc\n\n\ndef main():\n    # ===== Problem 4a =====\n    # Read in image1 as a grayscale image. Take the singular value\n    # decomposition of the image.\n\n    img1 = None\n\n    # BEGIN YOUR CODE HERE\n    from skimage import io\n    import numpy.linalg as la\n    img1 =io.imread('image1.jpg', as_gray=True)\n    U, Sigma, VT = la.svd() \n\n    # END YOUR CODE HERE\n\n    # ===== Problem 4b =====\n    # Save and display the best rank 1 approximation \n    # of the (grayscale) image1.\n\n    rank1approx = None\n\n    # BEGIN YOUR CODE HERE\n\n    # END YOUR CODE HERE\n\n    # ===== Problem 4c =====\n    # Save and display the best rank 20 approximation\n    # of the (grayscale) image1.\n\n    rank20approx = None\n\n    # BEGIN YOUR CODE HERE\n\n    # END YOUR CODE HERE\n\n\nif __name__ == '__main__':\n    main()",
    "Cursor": 421,
    "Symbol": "numpy.linalg",
    "Expected": "svd",
    "Provided": null
}
{
    "Buffer": "import math as ma\nimport numpy as nu\nimport sympy as sy\nfrom scipy import special as sp\nfrom matplotlib import pyplot as pl\nfrom matplotlib import cm\nfrom mpl_toolkits import mplot3d\npi = ma.pi\nrt = nu.sqrt\n\na = nu.linspace(1, 300, 100)\nn = 1/( 1+ 2/a*(1+rt(1+a)) )\n\npl.plot()\n",
    "Cursor": 270,
    "Symbol": "matplotlib.pyplot",
    "Expected": "plot",
    "Provided": null
}
{
    "Buffer": "'''\nMiscellaneous utility functions\n'''\nimport os\nimport shlex\nimport logging\nimport subprocess\n\nfrom subprocess import CalledProcessError\n\nLOG = logging.getLogger('SenseiModel')\nlogging.basicConfig(\n    format='%(asctime)s %(message)s',\n    datefmt='%m/%d/%Y %I:%M:%S %p',\n    level=logging.DEBUG)\n\n\ndef create_folder_if_not_exists(directory):\n    '''\n    Create the folder if it doesn't exist already.\n    '''\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\ndef run_command(command):\n    process = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE)\n    while True:\n        output = process.stdout.readline()\n        if output.endswith() and process.poll() is not None:\n            break\n        if output:\n            LOG.info(output.strip())\n    rc = process.poll()\n    return rc\n",
    "Cursor": 431,
    "Symbol": "os.path",
    "Expected": "exists",
    "Provided": null
}
{
    "Buffer": "import numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport month\r\n\r\ntimeperiod = 60\r\n\r\n\r\ng_cons = 1\r\nT_cons = 36\r\n\r\nfood = 100300\r\nrg = 0.029166666666666664\r\nrT = 0.01166666666666664\r\ng = 100000\r\nT = 300\r\nkillrate = 0.2\r\n\r\n\r\n\r\nGoat_pop =  month.model_loop(food, rg, rT, g, T, T_cons, g_cons, timeperiod, kil)[0]\r\nTort_pop =  month.model_loop(food, rg, rT, g, T, T_cons, g_cons, timeperiod, 0.2)[1]\r\nfinal_goat = month.model_loop(food, rg, rT, g, T, T_cons, g_cons, timeperiod, 0.2)[0][-1]\r\nfinal_tort = month.model_loop(food, rg, rT, g, T, T_cons, g_cons, timeperiod, 0.2)[1][-1]\r\n#\r\n# print(final_goat)\r\n# print(final_tort)\r\n\r\n###################\r\n# Matplotlib stuff behind here\r\n###################\r\ntime = np.arange(timeperiod)\r\ngoatplt = plt.plot(time, Goat_pop, \"b\")\r\ntortplt = plt.plot(time, Tort_pop, \"r\")\r\n# plt.axis([0, 365*5, 0, 100010])\r\nplt.show()\r\n",
    "Cursor": 747,
    "Symbol": "matplotlib.pyplot",
    "Expected": "plot",
    "Provided": null
}
{
    "Buffer": "import pygame\nimport ctypes\nimport os\nimport random\nfrom sim.simobjects import *\nimport datetime\n\ndef main(control, sim_length, frequency, dmin, dmax):\n  global screen, clock, reverse, lanes, road, cars, intersection, total_wait, count, \\\n  LANE_WIDTH, SPEED, SCREEN_SIZE\n\n  def _text_objects(text, font):\n    textSurface = font.render(text, True, BLUE)\n    return textSurface, textSurface.get_rect()\n\n  def message_display(text, center):\n    largeText = pygame.font.Font('freesansbold.ttf',70)\n    TextSurf, TextRect = _text_objects(text, largeText)\n    TextRect.center = (center)\n    screen.blit(TextSurf, TextRect)\n\n  def approx(p1, p2):\n    if abs(p1[0] - p2[0]) \u003c SPEED // 2:\n      if abs(p1[1] - p2[1]) \u003c SPEED // 2:\n        return True\n    return False\n\n  def rotate(direction, turn):\n    return {\n      'right': {\n        'up': 'right',\n        'right': 'down',\n        'down': 'left',\n        'left': 'up'\n      },\n      'left': {\n        'up': 'left',\n        'left': 'down',\n        'down': 'right',\n        'right': 'up'\n      }\n    } [turn] [direction]\n    \n\n  def get_turn(lane, turn):\n    return {\n      0: {\n        'right': 0,\n        'left': 2\n      },\n      2: {\n        'right': 1,\n        'left': 0\n      },\n      4: {\n        'right': 3,\n        'left': 1\n      },\n      6: {\n        'right': 2,\n        'left': 3\n      }\n    } [lane] [turn]\n\n  i = random.choice(range(1))\n  l = lanes[i]\n  car = Car(l.start, l.direction, SPEED, random.choice(['straight','right','left']), screen)\n  car.start = i\n  cars.add(car)\n  #total_wait = 0\n\n\n  for count in range(sim_length):\n    for event in pygame.event.get():\n      if event.type == pygame.QUIT:\n        pygame.quit()\n        quit()\n\n      elif event.type == pygame.KEYDOWN:\n        if event.key == pygame.K_q:\n          pygame.quit()\n          quit()\n\n    '''\n    i = 6\n    l = lanes[i]\n    g = random.choice(['straight', 'left', 'right'])\n    c = Car(l.start, l.direction, SPEED, g, screen)\n    c.start = i\n    cars.add(c)\n    '''\n    \n    if count % int(200 / SPEED) == 0:\n      i = random.choice(range(0, 7, 2))\n      l = lanes[i]\n      g = random.choice(['straight', 'left', 'right'])\n      c = Car(l.start, l.direction, SPEED, g, screen)\n      c.start = i\n      cars.add(c)\n    \n    for c in cars.sprites():\n      \n      c.speed = SPEED\n\n      for d in cars.sprites():\n        if d.rect.colliderect()\n      \n      if c.rect.colliderect(intersection.rect):\n        if not intersection.rect.contains(c.rect):\n          if intersection.flow != c.orientation and c.rect.colliderect(lanes[c.start]):\n            c.speed = 0\n          if reverse(c.orientation) in [d.orientation for d in intersection.incars] and c.rect.colliderect(lanes[c.start]):\n            c.speed = 0\n        else:\n          if c.goal != 'straight':\n            t = intersection.turns[get_turn(c.start, c.goal)]\n            if approx(c.rect.center, t) and not c.turned:\n              c.direction = rotate(c.direction, c.goal)\n              c.turned = True\n\n      if c.speed == 0:\n        total_wait += 1\n\n    control(frequency, dmin, dmax)\n\n\n    road.update(cars.sprites())\n    cars.update(road.sprites())\n    \n    screen.fill(YELLOW)\n\n    road.draw(screen)\n    cars.draw(screen)\n\n    \n    pygame.display.update()\n\n    clock.tick(180)\n\n    count += 1\n\nif __name__ == '__main__':\n\n  def timed(frequency, *args):\n    if count % frequency == 0:\n      intersection.flow = reverse(intersection.flow)\n\n  def custom(frequency, *args):\n    hlanes = [lanes[2], lanes[6]]\n    vlanes = [lanes[0], lanes[4]]\n    if count % frequency == 0:\n      intersection.flow = 'horizontal' if sum([len(l.cars) for l in hlanes]) \u003e sum([len(l.cars) for l in vlanes]) else 'vertical'\n    \n  def actuated(frequency, dmin, dmax):\n    global duration\n    def is_empty(lanes):\n      for l in lanes:\n        if len(l.cars) != 0:\n          return False\n      return True\n    hlanes = [lanes[2], lanes[6]]\n    vlanes = [lanes[0], lanes[4]]\n\n    switched = False\n    if count % frequency == 0:\n      switch = is_empty(hlanes) if intersection.flow == 'horizontal' else is_empty(vlanes)\n      if switch and duration \u003e dmin:\n        intersection.flow = reverse(intersection.flow)\n        switched = True\n        duration = 0\n      elif duration \u003e dmax:\n        intersection.flow = reverse(intersection.flow)\n        switched = True\n        duration = 0\n    if not switched:\n      duration += 1\n\n  reverse = lambda flow: {'horizontal': 'vertical'}.get(flow, 'horizontal')\n\n  user32 = ctypes.windll.user32\n  SCREEN_SIZE = user32.GetSystemMetrics(0), user32.GetSystemMetrics(1)\n  COMBINED_SCREEN_SIZE = user32.GetSystemMetrics(78), user32.GetSystemMetrics(79)\n  SECOND_SCREEN_SIZE = (COMBINED_SCREEN_SIZE[0] - SCREEN_SIZE[0], COMBINED_SCREEN_SIZE[1])\n  DISPLAY_MODE = \"single\" if COMBINED_SCREEN_SIZE == SCREEN_SIZE else \"dual\"\n  RATIO = 1.0\n  if DISPLAY_MODE == \"dual\":\n    DISPLAY_WIDTH, DISPLAY_HEIGHT = tuple([int(i // RATIO) for i in list(SECOND_SCREEN_SIZE)])\n    x, y = ((COMBINED_SCREEN_SIZE[0] + SCREEN_SIZE[0] - DISPLAY_WIDTH) // 2, (COMBINED_SCREEN_SIZE[1] - DISPLAY_HEIGHT) // 2)\n  else:\n    DISPLAY_WIDTH, DISPLAY_HEIGHT = tuple([int(i // RATIO) for i in list(SCREEN_SIZE)])\n    x, y = (SCREEN_SIZE[0] - DISPLAY_WIDTH) // 2, (SCREEN_SIZE[1] - DISPLAY_HEIGHT) // 2\n  os.environ['SDL_VIDEO_WINDOW_POS'] = \"%d,%d\" % (x,y)\n  \n  BLACK = (0, 0, 0)\n  WHITE = (255, 255, 255)\n  RED = (255, 0, 0)\n  GREEN = (0, 255, 0)\n  BLUE = (0, 0, 255)\n  YELLOW = (255, 255, 0)\n\n  LANE_WIDTH = int(DISPLAY_WIDTH * 0.1)\n  VERT_LANE_LENGTH = DISPLAY_HEIGHT // 2 - LANE_WIDTH\n  HORZ_LANE_LENGTH = (DISPLAY_WIDTH // 2 - LANE_WIDTH)\n\n  CENTER = (DISPLAY_WIDTH // 2, DISPLAY_HEIGHT // 2)\n\n\n  SPEED = 8\n  TRIALS = 1\n  SIM_LENGTH = 500\n\n  controls = [actuated, custom]\n  frequencies = [50]\n\n\n  for control in controls:\n    for frequency in frequencies:\n\n      count = 0\n      total_wait = 0\n      last_wait = 0\n      duration = 0\n      pygame.init()\n      screen = pygame.display.set_mode((DISPLAY_WIDTH, DISPLAY_HEIGHT))\n      pygame.display.set_caption('Simulation')\n\n\n      for i in range(TRIALS):\n\n        clock = pygame.time.Clock()\n        road = pygame.sprite.Group()\n        cars = pygame.sprite.Group()\n        intersection = Intersection(LANE_WIDTH * 2, LANE_WIDTH * 2, CENTER, screen)\n        road.add(intersection)\n        lanes = []\n        lanes.append(Lane(LANE_WIDTH, VERT_LANE_LENGTH, 'down', ((DISPLAY_WIDTH - LANE_WIDTH) // 2, (DISPLAY_HEIGHT - VERT_LANE_LENGTH) // 2 - LANE_WIDTH), screen))\n        lanes.append(Lane(LANE_WIDTH, VERT_LANE_LENGTH, 'up', ((DISPLAY_WIDTH + LANE_WIDTH) // 2, (DISPLAY_HEIGHT - VERT_LANE_LENGTH) // 2 - LANE_WIDTH), screen))\n        lanes.append(Lane(LANE_WIDTH, HORZ_LANE_LENGTH, 'left', ((DISPLAY_WIDTH + HORZ_LANE_LENGTH) // 2 + LANE_WIDTH, (DISPLAY_HEIGHT - LANE_WIDTH) // 2), screen))\n        lanes.append(Lane(LANE_WIDTH, HORZ_LANE_LENGTH, 'right', ((DISPLAY_WIDTH + HORZ_LANE_LENGTH) // 2 + LANE_WIDTH, (DISPLAY_HEIGHT + LANE_WIDTH) // 2), screen))\n        lanes.append(Lane(LANE_WIDTH, VERT_LANE_LENGTH, 'up', ((DISPLAY_WIDTH + LANE_WIDTH) // 2, (DISPLAY_HEIGHT + VERT_LANE_LENGTH) // 2 + LANE_WIDTH), screen))\n        lanes.append(Lane(LANE_WIDTH, VERT_LANE_LENGTH, 'down', ((DISPLAY_WIDTH - LANE_WIDTH) // 2, (DISPLAY_HEIGHT + VERT_LANE_LENGTH) // 2 + LANE_WIDTH), screen))\n        lanes.append(Lane(LANE_WIDTH, HORZ_LANE_LENGTH, 'right', ((DISPLAY_WIDTH // 2 - LANE_WIDTH) // 2, (DISPLAY_HEIGHT + LANE_WIDTH) // 2), screen))\n        lanes.append(Lane(LANE_WIDTH, HORZ_LANE_LENGTH, 'left', ((DISPLAY_WIDTH // 2 - LANE_WIDTH) // 2, (DISPLAY_HEIGHT - LANE_WIDTH) // 2), screen))\n        road.add(*lanes)\n        \n        main(control, SIM_LENGTH, frequency, 40, 150)\n\n        print(f'\\nWait Time: {total_wait - last_wait}')\n        last_wait = total_wait\n\n      print(f'\\nAverage frames waited: {total_wait // TRIALS}')\n      \n      pygame.quit()\n\n  quit()",
    "Cursor": 1677,
    "Symbol": "pygame",
    "Expected": "quit",
    "Provided": null
}
{
    "Buffer": "import numpy as np\nimport cv2\n\nfrom model.mask_A2D.mask_rcnn.core.config import cfg\nimport model.mask_A2D.mask_rcnn.utils.blob as blob_utils\nimport model.mask_A2D.mask_rcnn.roi_data.rpn\nfrom model.mask_A2D.mask_rcnn import roi_data\nimport os\n\n\ndef get_minibatch_blob_names(is_training=True):\n    \"\"\"Return blob names in the order in which they are read by the data loader.\n    \"\"\"\n    # data blob: holds a batch of N images, each with 3 channels\n    blob_names = ['data']\n    if cfg.RPN.RPN_ON:\n        # RPN-only or end-to-end Faster R-CNN\n        blob_names += roi_data.rpn.get_rpn_blob_names(is_training=is_training)\n    elif cfg.RETINANET.RETINANET_ON:\n        raise NotImplementedError\n    else:\n        # Fast R-CNN like models trained on precomputed proposals\n        blob_names += roi_data.fast_rcnn.get_fast_rcnn_blob_names(\n            is_training=is_training\n        )\n    return blob_names\n\n\ndef get_minibatch(roidb, frame_root):\n    \"\"\"Given a roidb, construct a minibatch sampled from it.\"\"\"\n    # We collect blobs from each image onto a list and then concat them into a\n    # single tensor, hence we initialize each blob to an empty list\n    blobs = {k: [] for k in get_minibatch_blob_names()}\n\n    # Get the input image blob\n    im_blob, im_scales = _get_image_blob(roidb, frame_root)\n    blobs['data'] = im_blob\n    if cfg.RPN.RPN_ON:\n        # RPN-only or end-to-end Faster/Mask R-CNN\n        valid = roi_data.rpn.add_rpn_blobs(blobs, im_scales, roidb)\n    elif cfg.RETINANET.RETINANET_ON:\n        raise NotImplementedError\n    else:\n        # Fast R-CNN like models trained on precomputed proposals\n        valid = roi_data.fast_rcnn.add_fast_rcnn_blobs(blobs, im_scales, roidb)\n    return blobs, valid\n\n\ndef _get_image_blob(roidb, frame_root):\n    \"\"\"Builds an input blob from the images in the roidb at the specified\n    scales.\n    \"\"\"\n    num_images = len(roidb)\n    # Sample random scales to use for each image in this batch\n    scale_inds = np.random.randint(\n        0, high=len(cfg.TRAIN.SCALES), size=num_images)\n    processed_ims = []\n    im_scales = []\n    for i in range(num_images):\n        im = cv2.imread(os.path.join(frame_root, roidb[i]['image'] + '.png'))\n        assert im is not None, \\\n            'Failed to read image \\'{}\\''.format(roidb[i]['image'])\n        # If NOT using opencv to read in images, uncomment following lines\n        # if len(im.shape) == 2:\n        #     im = im[:, :, np.newaxis]\n        #     im = np.concatenate((im, im, im), axis=2)\n        # # flip the channel, since the original one using cv2\n        # # rgb -\u003e bgr\n        # im = im[:, :, ::-1]\n        if roidb[i]['flipped']:\n            im = im[:, ::-1, :]\n        target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n        im, im_scale = blob_utils.prep_im_for_blob(\n            im, cfg.PIXEL_MEANS, [target_size], cfg.TRAIN.MAX_SIZE)\n        im_scales.append(im_scale[0])\n        processed_ims.append(im[0])\n\n    # Create a blob to hold the input images [n, c, h, w]\n    blob = blob_utils.im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\ndef _get_flow_blob(roidb, )",
    "Cursor": 2147,
    "Symbol": "os.path",
    "Expected": "join",
    "Provided": null
}
{
    "Buffer": "variable = 10\n\ndef hello():\n    print(\"Hello World!\")\n\nimport os\nfrom os import posixpath\nimport matplotlib\nimport json\nimport posix\nimport csv\nimport requests\nimport subprocess\nimport boto\nimport sqlalchemy\n\nengine = sqlalchemy.create_engine()\n\nsqlalchemy.ext.declarative.api.declarative_base\n\njson.dumps()\n\nboto.s3.connection.S3Connection()\n\nposix.chmod\nos.mkdir\ncsv.reader\nmatplotlib.pyplot.plot()\n\nrequests.get()\n\n__builtins__.list\n\nclass A:\n    def __init__(self, foo):\n        self.foo = foo\n\n    def increment(self, n):\n        return n + 1\n\nclass B(A):\n    def __init__(self, foo):\n        super(foo)\n\n    def increment(self, n, **kwargs):\n        inc = kwargs.delta or 1\n        return super(B, self).increment(abc) + inc\n\n    def dummy(self, abc=True, fed, ghi, jkl, mno, pqr, stuv):\n        return abc\n\nif True:\n    test = B('foo')\nelse:\n    test = A('foo')\n\ntest.increment(2, delta=5)\ntest.increment('bar')\ntest.increment('baz')\ntest.increment(False) if True else test.increment(True)\n\ntest.dummy()\n\nif __name__ == \"__main__\":\n    hello()\n\nmap(filter(123\n\nimport ji",
    "Cursor": 411,
    "Symbol": "requests",
    "Expected": "get",
    "Provided": null
}
{
    "Buffer": "import datetime\nimport logging\n\nfrom django import forms\n\nfrom celebro.base.utility import sum_time, confronto_orari\nfrom celebro.ticket.models import (\n    Carico,\n    FasciaOraria,\n    ScaricoPrenotazione,\n    Biglietteria,\n)\n\nlog = logging.getLogger(__name__)\n\n\nclass NuovoScaricoPrenotazioneForm(forms.ModelForm):\n    \"\"\"Classe per il form di prenotazione dei resi\n    \"\"\"\n\n    biglietteria = forms.ModelChoiceField(\n        queryset=Biglietteria.objects.filter(postazione_scarico=True)\n    )\n\n    def clean(self):\n        super()\n        try:\n            fascia_oraria = self.cleaned_data[\"fascia_oraria\"]\n        except KeyError:\n            raise forms.ValidationError(\"Bisogna inserire una fascia oraria\")\n\n        try:\n            data_ritiro = self.cleaned_data[\"data_ritiro\"]\n        except KeyError:\n            raise forms.ValidationError(\"Bisogna inserire una data di ritiro\")\n\n        if ScaricoPrenotazione.objects.filter(data_ritiro=data_ritiro).count() \u003e= 2:\n            raise forms.ValidationError(\"Per la giornata scelta è già piena\")\n\n        if data_ritiro.weekday() == 5 or data_ritiro.weekday() == 6:\n            raise forms.ValidationError(\"Puoi solo scegliere da lunedi a venerdi\")\n\n        # validate piece\n        if data_ritiro == datetime.date.today():\n            now = datetime.datetime.now().strftime(\"%H:%M:%S\")\n            limit = sum_time(now, \"00:30:00\")\n            if confronto_orari(limit, fascia_oraria.nome):\n                raise forms.ValidationError(\n                    \"Puoi prenotare solo dalle \" + limit + \" di oggi in poi\"\n                )\n\n        return self.cleaned_data\n\n    def clean_data_ritiro(self):\n        data_ritiro = self.cleaned_data[\"data_ritiro\"]\n        if data_ritiro is None:\n            raise forms.ValidationError(\"Bisogna inserire una data di ritiro\")\n        if data_ritiro \u003c datetime.date.today():\n            raise forms.ValidationError(\"La data non puo' essere nel passato\")\n        return data_ritiro\n\n    class Meta:\n        model = ScaricoPrenotazione\n        exclude = [\"user\"]\n\n\nclass NuovoCaricoForm(forms.ModelForm):\n    \"\"\"Classe per il form dei carichi\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.request = kwargs.pop(\"request\", None)\n        super().__init__(*args, **kwargs)\n        self.fields[\"fascia_oraria\"].queryset = FasciaOraria.objects.none()\n        self.fields[\"biglietteria\"].queryset = permitted_biglietterie_carico()\n\n        if \"biglietteria\" in self.data:\n            try:\n                self.fields[\"fascia_oraria\"].queryset = FasciaOraria.objects.all()\n            except (ValueError, TypeError):\n                pass\n\n    def clean(self):\n        super()\n        try:\n            fascia_oraria = self.cleaned_data[\"fascia_oraria\"]\n        except KeyError:\n            raise forms.ValidationError(\"Bisogna inserire una fascia oraria\")\n\n        biglietteria = self.cleaned_data[\"biglietteria\"]\n        fascie_orarie = biglietteria.orari_disponibili.all()\n        if \"ticket.notturno_aereoporto\" not in self.request.user.get_all_permissions():\n            log.info(\"Non puoi vedere i notturni\")\n            fascie_orarie = fascie_orarie.exclude(notturno=True)\n\n        if not fascia_oraria in fascie_orarie:\n            raise forms.ValidationError(\"La fascia oraria scelta non è valida\")\n\n        try:\n            data_ritiro = self.cleaned_data[\"data_ritiro\"]\n        except KeyError:\n            raise forms.ValidationError(\"Bisogna inserire una data di ritiro\")\n\n        # validate piece\n        if data_ritiro == datetime.date.today():\n            now = datetime.datetime.now().strftime(\"%H:%M:%S\")\n            limit = sum_time(now, \"00:30:00\")\n            if confronto_orari(limit, fascia_oraria.nome):\n                raise forms.ValidationError(\n                    \"Puoi prenotare solo dalle \" + limit + \" di oggi in poi\"\n                )\n\n        return self.cleaned_data\n\n    def clean_data_ritiro(self):\n        data_ritiro = self.cleaned_data[\"data_ritiro\"]\n        if data_ritiro == None:\n            raise forms.ValidationError(\"Bisogna inserire una data di ritiro\")\n        if data_ritiro \u003c datetime.date.today():\n            raise forms.ValidationError(\"La data non puo' essere nel passato\")\n        return data_ritiro\n\n    class Meta:\n        model = Carico\n        exclude = [\"biglietti_ordinati\", \"user\", \"status\", \"conteggio_stats\"]\n\n\nclass NuovoCaricoAdminForm(forms.ModelForm):\n    \"\"\"Classe per il form dei carichi per gli admin\n    \"\"\"\n\n    biglietteria = forms.ModelChoiceField(\n        queryset=Biglietteria.objects.filter(postazione_carico=True)\n    )\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.fields[\"fascia_oraria\"].queryset = FasciaOraria.objects.none()\n\n        if \"biglietteria\" in self.data:\n            try:\n                self.fields[\"fascia_oraria\"].queryset = FasciaOraria.objects.all()\n            except (ValueError, TypeError):\n                pass\n\n    def clean(self):\n        super()\n        try:\n            fascia_oraria = self.cleaned_data[\"fascia_oraria\"]\n        except KeyError:\n            raise forms.ValidationError(\"Bisogna inserire una fascia oraria\")\n\n        try:\n            data_ritiro = self.cleaned_data[\"data_ritiro\"]\n        except KeyError:\n            raise forms.ValidationError(\"Bisogna inserire una data di ritiro\")\n        return self.cleaned_data\n\n    def clean_data_ritiro(self):\n        data_ritiro = self.cleaned_data[\"data_ritiro\"]\n        if data_ritiro == None:\n            raise forms.ValidationError(\"Bisogna inserire una data di ritiro\")\n        if data_ritiro \u003c datetime.date.today():\n            raise forms.ValidationError(\"La data non puo' essere nel passato\")\n        return data_ritiro\n\n    class Meta:\n        model = Carico\n        exclude = [\"biglietti_ordinati\", \"status\"]\n",
    "Cursor": 836,
    "Symbol": "django.forms",
    "Expected": "ValidationError",
    "Provided": null
}
{
    "Buffer": "# coding=utf-8\nimport sc\nfrom AFC.sdi import dao_sdi\n\nSTATO_INVIATO = 'inviato'\nSTATO_VALIDO = 'valido'\nSTATO_ERRORE = 'errore'\nSTATI_SDI = [STATO_VALIDO, STATO_INVIATO, STATO_ERRORE]\nTIPO_TRACCIATO = 'J_FI_FE'\nclass TrasmissioneSdi(sc.BaseObject):\n\t\n\tsocieta = sc.Property(\"societa\", default='')\n\tid_trasmissione = sc.Property(\"id_trasmissione\", default='', validator=sc.validate_int)\n\t\n\t# SOCIETA_SAP + PROGRESSIVO 12CH\n\tid_transazione = sc.Property(\"id_transazione\", default='')\n\t\n\tid_documento = sc.Property(\"id_documento\", default='')\n\t\n\t\n\ttipo_tracciato = sc.Property(\"tipo_tracciato\", default=TIPO_TRACCIATO)\n\tstato = sc.Property(\"stato\", default=STATO_VALIDO, choices=STATI_SDI)\n\t\n\ttesto_libero = sc.Property(\"testo_libero\", default='')\n\tfeedback_magnum = sc.Property(\"feedback_magnum\", default='')\n\tdescrizione_feedback = sc.Property(\"descrizione_feedback\", default='')\n\tsegnalazione_errore = sc.Property(\"segnalazione_errore\", default='')\n\tdataora_invio = sc.Property(\"dataora_invio\", default='')\n\ttracciato_inviato = sc.Property(\"tracciato_inviato\", default='')\n\tnumero_idoc = sc.Property(\"numero_idoc\", default='')\n\tdata_ora_reinvio = sc.Property(\"data_ora_reinvio\", default='')\n\tutente_reinvio = sc.Property(\"utente_reinvio\", default='')\n\tfeedback_flusso_attivo = sc.Property(\"feedback_flusso_attivo\", default='')\n\tdata_flusso_attivo = sc.Property(\"data_flusso_attivo\", default='')\n\t\n\tins_user = sc.Property(\"ins_user\", default='')\n\tupd_user = sc.Property(\"upd_user\", default='')\n\t\n\t\n\tdef __init__(self, data, pard, from_db=False):\n\t\tsuper(TrasmissioneSdi, self).__init__(data, pard, from_db)\n\t\tself.id_transazione = TrasmissioneSdi.get_id_transazione(pard, pard['societa']) #questo? è bene lasciarlo qui o no? #no\n\t\t\n\t\n\t@staticmethod\n\tdef get_fast(pard, params):\n\t\t\n\t\tif params.get('id_documento'):\n\t\t\tpass\n\t\telif params.get('id_trasmissinoe'):\n\t\t\tpass\n\t\telif params.get('id_transazione'):\n\t\t\tpass\n\t\telse:\n\t\t\traise sc.ErrorList(\"TrasmissioneSdi: Chiave logica non valida. ({})\".format(params))\n\t\t\n\t\treturn dao_sdi.get_trasmissione(pard, params)\n\t\n\t@staticmethod\n\tdef get_all_by_id_documento(pard, id_documento):\n\t\ttrasmissioni = TrasmissioneSdi.get_fast(pard, {'id_documento': id_documento})\n\t\tlista_trasmissioni_obj = [TrasmissioneSdi(i, pard) for i in trasmissioni]\n\t\treturn lista_trasmissioni_obj\n\t\n\t@staticmethod\n\tdef get_id_transazione(pard, societa):\n\t\tdao_sdi.get_id_transazione()\n\t\treturn societa + '1'.zfill(12)\n\t\n\t\n\tdef salva(self): \n\t\tpass\n\t\n\tdef insert_trasmissione(self):\n\t\tparams = self.to_dict()\n\t\tret = dao_sdi.insert_sdi_trasmissione(pard, params)\n\t\treturn ret",
    "Cursor": 1836,
    "Symbol": "__builtin__.dict",
    "Expected": "get",
    "Provided": null
}
{
    "Buffer": "import threading\nimport json\nimport time\nimport csv\nimport pandas as pd\nimport numpy as np\nimport concurrent\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nfrom utils import get_intersection, get_total, turn_handles_into_ids\n\n\ndf_esports = pd.read_csv('all_esports.csv')\nesports_handles = set(df_esports.handles)\nesports_handles.update(set(df_esports.rights_holder))\ncasters = ['overwatchleague', 'natenanzer', 'montecristo', 'ggdoa', 'mattmrx',\n            'ubershouts', 'mlgpuckett', 'soembie', 'onfiresemmler', 'prophetcrumbz',\n            'reinforce', 'hexagrams', 'brencasts', 'sideshowgaming', 'goldenboyftw',\n            'fortnitegame', 'playoverwatch', 'leagueoflegends', 'lolesports',\n            'dota2', 'csgo_dev', 'twitch', 'callofduty',\n            ]\nesports_handles.update(casters)\n\nexceptions = set(df_esports[df_esports.game_type == 'nba2k'].rights_holder)\nexceptions.update(['jlo', 'shaq', 'mrodofficial', 'arod', 'kingbach', 'amandacerny', 'ramsnfl', 'oliviamunn',\n             'drake', 'tiesto', 'johnlegere', 'nestle', 'marsglobal', 'iamakademiks', 'romanatwood',\n             'natalieevamarie', 'barondavis', 'postmalone', 'hoodieallen', 'michaelstrahan', 'ricegum',\n             'teamjuju', 'trevernoah', 'fousey', 'marshmellomusic', 'snoopdogg', 'l0wtiergod', 'macmiller',\n             'katyperry', 'zedd', 'skrillex', 'steveaoki', 'KDTrey5', 'KingJames', 'KevinHart4real', 'pewdiepie',\n             'Eminem', 'kanyewest', 'ArianaGrande', 'Beyonce', 'nickiminaj', 'taylorswift13',\n             'thenotoriousmma', 'serenawilliams', 'neymarjr', 'ramsnfl',\n             'mcuban', 'garyvee', 'caseyneistat', 'tferriss', 'bulletproofexec', 'daveramsey', 'jimcramer',\n             'mike_stelzner', 'thisissethsblog', 'patflynn', 'kevinolearytv', 'robertherjavec', 'lorigreiner',\n             'thesharkdaymond', 'barbaracorcoran', 'codinghorror', 'labunleashed', 'elonmusk', 'susanwojcicki',\n             'guyraz', 'jockowillink', 'tonyrobbins', 'michaelhyatt', 'mikeroweworks', 'romanmars', 'billburr',\n             'adamcarolla', 'joerogan', 'smallbizlady', 'tailopez', 'taliagold', 'calvinharris', 'arminvanbuuren',\n             'martingarrix', 'hardwell', 'davidguetta', 'dillonfrancis', 'deadmau5', 'axwell', 'disclosure', 'avicii',\n             'diplo', 'djkhaled', 'lifeofdesiigner', 'tydollasign', 'tyga', 'kendricklamar', 'trvisxx', 'chancetherapper',\n             'donaldglover', 'NickyDiamonds', 'bobbyhundreds', 'virgilabloh', 'trvisXX', 'DonKe713', 'LILUZIVERT', 'wex1200',\n             ]\n)\nfor i in exceptions:\n    try:\n        esports_handles.remove(i.lower().strip())\n    except:\n        print(i)\n\ngames = [\n    'league-of-legends', 'gears-of-war', 'arena-of-valor', 'fortnite', 'counter-strike',\n    'playerunknown-battlegrounds', 'starcraft2', 'nba2k', 'paladins', 'overwatch',\n    'clash-royale', 'heroes-of-the-storm', 'hearthstone', 'fifa', 'rocket-league',\n    'smite', 'halo', 'call-of-duty', 'smash-brothers', 'street-fighter', 'FGC',\n    'rainbow-six-siege', 'battle-royale', 'dota2',\n]\ngames = ['rocket-league', 'nba2k']\ngame_mapping = {'playerunknown-battlegrounds': 'PUBG',\n                'starcraft2': 'starcraft',\n                }\n\nhandles_left, left_names, handles_right, right_names = [], [], [], []\n\nfor game in games:\n    if game == 'FGC':\n        handles = set(df_esports[df_esports.game_type.isin(['street-fighter', 'tekken7', 'smash-brothers'])])\n    else:\n        handles = set(df_esports[df_esports.game_type == game].handles)\n    if game in game_mapping:\n        game = game_mapping[game]\n    try:\n        df = pd.read_excel('esports_members.xlsx', sheet_name=game)\n        handles_extra = set(df['Twitter Handle'].str.lower().str.strip())\n        handles_extra = {str(i).replace('@', '').replace('\\u200f', '').strip() for i in handles_extra}\n        handles.update(handles_extra)\n    except:\n        print('Not in xlsx game', game)\n\n    handles = handles - exceptions\n    print('cavs' in handles)\n    handles_left.append(handles)\n    left_names.append(f'{game} Universe')\n    ids = turn_handles_into_ids(handles)\n\n    print(game, 'Handle Count:', len(handles), len(ids))\n\nhandles_left.append(esports_handles)\nleft_names.append('Whole Esports Universe')\n\n\nbrands = ['US Airlines']\n\nfor brand in brands:\n    df = pd.read_excel('travel_and_hotel.xlsx', sheet_name=brand)\n    handles = set(df['Twitter Handle'])\n    handles = {i.replace('@', '').strip(for i in handles}\n    handles_right.append(brand)\n    right_names.append(f'@{brand}')\n    # print(brand, len(handles), len(turn_handles_into_ids(handles)))\n\n\ndef compare2things(\n    handles_left: iter, handles_right: iter, left_name: str, right_name: str\n        ):\n    left_ids = turn_handles_into_ids(handles_left)\n    left_amount = get_total(left_ids)\n    right_ids = turn_handles_into_ids(handles_right)\n    right_amount = get_total(right_ids)\n    inner_amount = get_intersection(left_ids, right_ids)\n\n    return str(left_name), str(right_name), left_amount, inner_amount, right_amount\n\n\nwith ThreadPoolExecutor(max_workers=None) as executor:\n    jobs = []\n    for handle_left, left_name in zip(handles_left, left_names):\n        for handle_right, right_name in zip(handles_right, right_names):\n            print(left_name, len(handle_left), 'vs', right_name, len(handle_right))\n            future = executor.submit(compare2things, handle_left, handle_right, left_name, right_name)\n            jobs.append(future)\n\n    with open('puma_vs_esports.csv', 'w') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['left_name', 'right_name', 'left_amount', 'inner_amount', 'right_amount'])\n        for future in concurrent.futures.as_completed(jobs):\n            result = future.result()\n            print(result)\n            writer.writerow(result)\n",
    "Cursor": 5526,
    "Symbol": "csv",
    "Expected": "writer",
    "Provided": null
}
