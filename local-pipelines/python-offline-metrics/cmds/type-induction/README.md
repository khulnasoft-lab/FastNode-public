# Model
## Definition
We attempt to learn the return types of functions through [EM algorithm](http://cs229.stanford.edu/notes/cs229-notes8.pdf) 
by examining code samples of the form:
```
v = f() // Function call
v.attr1 // Access attributes
...
v.attrk
```

We represent this with the notation:
- `s = (f, v, (a1,...,ak))`
- `D = {s1,...,sn}`
- let `f` be a random variable with support on the set of valid functions
- let `t` be a random variable with support on the set of valid types where `t` is the type of the
  return value of `f`
- let `a` be a random variable with support over the set of lists of valid attributes,
  e.g `a = (a1,...,ak)` where `a1,...ak` is a list of valid attributes
- for simplicity we assume that all samples have exactly `k` attributes, this turns out
  not to matter for our purposes since we only care about estimating the return type of each `f`

Our generative model of each sample `s` is as follows:
- sample `f` from `P(f)`
- sample `t` from `P(t|f)`
- for `i` in `{1,...,k}`:
  - sample `ai` from `P(a|t)`

This is equivalent to modeling the joint distribution as:
`P(t,f,a) = P(f)P(t|f)P(a|t)`

In words, each sample `s` is generated as:
- choose a function `f` 
- choose a type `t` for the return value of function `f` and assign a value of type `t` to the variable `v`
- choose a set of attributes to access on the return value of `f` from the distribution `P(a|t)`

## Learning
The type `t` of the return value of each function is unknown (equivalently the type of each `v` is unknown), 
we observe the value of `f` and the value of `a`.

We use the EM algorithm to learn `P(t,f,a;B)`, where `B` denotes the parameters of our model.

# Pipelines
Running `make` will do the following:
- `make traindata` will generate training data for the packages listed in `packges.txt`
- `make train` will train an EM model for each package.
	- Starting from `__builtin__`, all packages are trained in a `sequence`, 
	where package`sequence[i]` only depends on `sequence[:i-1]`, all `P(a|t)` are saved in files.
	- When training a model for a new package, the pretrained `P(a|t)` of its dependencies will be loaded.
	- The order of training is determined by the order in `pacakges.txt`
- `make validatedata` will generate validation examples from segment data.
- `make validate` will validate the model using the examples and save the report

The part of generating training samples can also be done in distributed mode 
by doing `make dist_traindata` and the samples will be saved to s3 directories. 
See [here](https://fastnode.quip.com/bRd8AR3xv9xq/Launching-a-distributed-pipeline) for details of distributed pipelines.

# Metadata
The metadata used in the pipelines:
- `deps.json` is a map from a package to its dependencies, 
it comes from the package exploration results under `.metadata.requires` field.
- `all-packges.txt` and `packages.txt` are generated by `make order` under
`local-pipelines/python-cluster-packages`.
	- `all-packges.txt` is a list of all possible packages we've seen in github
	data, segment data and dependencies. It's used for extracting training samples. 
	(Not all packages will have training samples, it's just a starting list.)
	- `packages.txt` is a list of packages that we want to train on, it includes some
	standard libs, the packages that appear frequent enough. And they are sorted in an order
	of dependency, every package only depends on the ones in front of it.